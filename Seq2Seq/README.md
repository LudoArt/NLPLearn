存在的问题：梯度爆炸

梯度爆炸解决的主要思路：减小batch_size，使用梯度截断，权重正则化

但仍未解决问题

~~可尝试减小输入的数据，将之前的整数除以字典长度~~

问题已解决，解决方法：在encoder中定义的目标词汇长度为4000（TRG_VOCAB_SIZE = 4000），但在构建字典的时候，中文的词汇量也按照1000来进行计算，重新构建字典，将中文词汇控制在4000，梯度爆炸问题解决。

那么问题来了，为什么这样就解决了？？？

